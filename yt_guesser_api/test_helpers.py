from yt_guesser_api.chalicelib.helpers import TextFinder, ApproximateMap

SAMPLE_ENRICHED_TRANSCRIPT = '''
- OpenAI just dropped GPT-4, and as cool as it is and as much as it's improving things, it still doesn't solve the biggest problem with AI.
- And until this problem is solved, we can be pretty confident our jobs aren't going anywhere.
- The problem is hallucination. What am I talking about when I say hallucination with AI? Like, it's a robot, it can't imagine things. That's kind of the problem.
- Robots in AI can't tell the difference between the truth and something it made up incorrectly from information that it has.
- And when it shares this information with you, it will approach a lie and the truth with the exact same levels of confidence.
- AI isn't smart enough to know which information to combine correctly or incorrectly. It just takes all of the information, combines all of that into something, and then gives you a result based on that.
- Because of this, it can and will combine information wrong. I was actually really pumped to see how much they call it out in this update. They actually coined the term hallucination, and it's what I'll be using to describe this problem forever now. It hallucinates facts and makes errors, and we'll spit those out with confidence.
- So, if you're using AI as a learning tool, you don't know when it's lying to you or not until you've applied the information and it fails.
- It's really hard to trust AI as a method for learning new information. It's the confidence that spits out its incorrect answers with makes it inherently flawed.
- And as we see here, things have gotten better, and it depends a lot on the field. Like, GPT-2 had a 50% accuracy rating, and GPT-4's up to 80-ish for learning.
- But when we move to something like math, we see a bigger jump, still under 70%. Or if we go to code, we see a decent-sized jump, but we're still under 70%.
- That's why it's hard for me to recommend AI for beginners and new developers. And as cool as chat GPT is, you have to have a very untrusting relationship with it.
- You can't rely on the AI's answers. You have to use it as a tool and a resource that you know is wrong as often as it isn't. And yes, it's only wrong 30% of the time here, but you don't know if you're in the 30% or not at any given time.
- So, you have to treat every answer AI gives you as incorrect. That's scary, and I don't think we talk about this problem enough.
- It is a fantastic tool for streamlining things. It is a good resource of ideas and concepts and a way to reduce ideas down to something real.
- When it comes down to facts and information and the things we rely on to learn and grow and develop, it doesn't provide a source of truth we can rely on. And that's what's scary.
- In a future where more and more information is generated by AI, and we have more and more AI learning from information generated from AI, this fake information is going to accelerate like mad.
- Because if one AI is wrong about a thing and an article comes out about that incorrect thing, another AI can read it and think it's within that 70% of truth.
- But in that 30% of truth, it's reading lies some percentage of the time. That is terrifying.
- And we have to be very careful with labeling information that isn't AI-generated as a result because otherwise, we'll be risking diluting information down to a set of vaguely sometimes correct facts.
- Hallucination will kill AI if we don't find good ways around it. In this problem, although improving, has not yet been solved. And until it is, I can confidently say your jobs are safe.
- AI will just make it easier to do certain parts of them, but your knowledge and your ability to identify correct and incorrect will continue to make you valuable as an engineer.
- You've already checked out my other video about AI. I will pin that one right here. It's pretty solid. We talk all about how you can use AI to be better as an engineer today.
- I don't think we should be scared about tomorrow. Hope this was helpful. Peace.
'''

tf = TextFinder()

def test_find_text():
    s = 'This is a test string'
    text = 'This is a test string'
    assert tf.find_text(s, text) == 0

def test_find_snippet1():
    s = 'open AI just dropped gpt4 and as cool as'
    assert tf.find_text(s, SAMPLE_ENRICHED_TRANSCRIPT) == 3

def test_find_snippet2():
    s = "it is and as much as it's improving"
    assert tf.find_text(s, SAMPLE_ENRICHED_TRANSCRIPT) == 45

# Test the approximate map
def test_approximate_map():
    amap = ApproximateMap()
    amap.add(10, 1)
    amap.add(20, 2)
    amap.add(29.9, 3.1)

    assert amap.get_lt(10) == None
    assert amap.get_lt(11) == (10, 1)
    assert amap.get_lteq(9) == None
    assert amap.get_lteq(10) == (10, 1) 
    assert amap.get_gt(30) == None
    assert amap.get_gt(29) == (29.9, 3.1) 
    assert amap.get_gteq(31) == None
    assert amap.get_gteq(29.9) == (29.9, 3.1)